{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5133affd",
   "metadata": {},
   "source": [
    "# A2c and reinforce\n",
    "* https://colab.research.google.com/github/yfletberliac/rlss-2019/blob/master/labs/DRL.01.REINFORCE%2BA2C.ipynb#scrollTo=Yx0xvpJeyFel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b5e02ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in d:\\anaconda3\\lib\\site-packages (7.6.5)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipywidgets) (5.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from ipywidgets) (1.0.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipywidgets) (6.0.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in d:\\anaconda3\\lib\\site-packages (from ipywidgets) (3.5.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in d:\\anaconda3\\lib\\site-packages (from ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipywidgets) (7.25.0)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
      "Requirement already satisfied: debugpy>=1.0.0 in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=4.0.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=18.5 in d:\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (61.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=4.0.0->ipywidgets) (3.0.19)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=4.0.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=4.0.0->ipywidgets) (5.0.9)\n",
      "Requirement already satisfied: pygments in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=4.0.0->ipywidgets) (2.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: fastjsonschema in d:\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in d:\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from nbformat>=4.2.0->ipywidgets) (4.7.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in d:\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in d:\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.4.8)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (22.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in d:\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.13.1)\n",
      "Requirement already satisfied: nbconvert in d:\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.4.4)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in d:\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.5)\n",
      "Requirement already satisfied: argon2-cffi in d:\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in d:\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.1.2)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in d:\\anaconda3\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets) (304)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jerry john thomas\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in d:\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in d:\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in d:\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in d:\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.13)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in d:\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.11.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in d:\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: bleach in d:\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in d:\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: testpath in d:\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: defusedxml in d:\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.3.1)\n",
      "Requirement already satisfied: packaging in d:\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3)\n",
      "Requirement already satisfied: webencodings in d:\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81dab6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvirtualdisplay in d:\\anaconda3\\lib\\site-packages (3.0)\n",
      "Requirement already satisfied: pyglet==1.5.27 in d:\\anaconda3\\lib\\site-packages (1.5.27)\n",
      "Collecting imageio-ffmpeg\n",
      "  Downloading imageio_ffmpeg-0.4.8-py3-none-win_amd64.whl (22.6 MB)\n",
      "Installing collected packages: imageio-ffmpeg\n",
      "Successfully installed imageio-ffmpeg-0.4.8\n"
     ]
    }
   ],
   "source": [
    "! pip install pyvirtualdisplay\n",
    "# ! pip install pyglet\n",
    "! pip install pyglet==1.5.27\n",
    "! pip install imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e15e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "import random, os.path, math, glob, csv, base64, itertools, sys\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from pprint import pprint\n",
    "\n",
    "# The following code is will be used to visualize the environments.\n",
    "\n",
    "def show_video(directory):\n",
    "    html = []\n",
    "    for mp4 in Path(directory).glob(\"*.mp4\"):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append('''<video alt=\"{}\" autoplay \n",
    "                      loop controls style=\"height: 400px;\">\n",
    "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "#     ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start();\n",
    "\n",
    "def make_seed(seed):\n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3aac851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "def show_video(directory):\n",
    "    html = []\n",
    "    for mp4 in Path(directory).glob(\"*.mp4\"):\n",
    "        Video(mp4)\n",
    "        filename = \"./gym-results/openaigym.video.0.14036.video000000.mp4\"\n",
    "        filename=mp4\n",
    "        print(\"file name \", mp4)\n",
    "        video = io.open(filename, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        HTML(data='''<video alt=\"test\" controls>\n",
    "                        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))\n",
    "        \n",
    "#         video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "#         html.append('''<video alt=\"{}\" autoplay \n",
    "#                       loop controls style=\"height: 400px;\">\n",
    "#                       <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "#                  </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "#     ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "    \n",
    "# display = Display(visible=0, size=(1400, 900))\n",
    "# display = Display(visible=True)\n",
    "# display.start();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6817c7ac",
   "metadata": {},
   "source": [
    "Pytorch recap\n",
    "\n",
    "| Component | Description |\n",
    "| ---- | --- |\n",
    "| [**torch**](https://pytorch.org/docs/stable/torch.html) | a Tensor library like NumPy, with strong GPU support |\n",
    "| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n",
    "| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n",
    "| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | a neural networks library deeply integrated with autograd designed for maximum flexibility |\n",
    "| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n",
    "| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a7ab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.zeros(5,3)\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.eye(3)\n",
      " tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "torch.tensor(np.eye(3))\n",
      " tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]], dtype=torch.float64)\n",
      "torch.tensor(np.eye(3)).numpy()\n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "torch.randint(0,10,(2,3))\n",
      " tensor([[2, 6, 4],\n",
      "        [8, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.zeros(5,3)\\n\",torch.zeros(5,3))\n",
    "print(\"torch.eye(3)\\n\",torch.eye(3))\n",
    "print(\"torch.tensor(np.eye(3))\\n\",torch.tensor(np.eye(3)))\n",
    "print(\"torch.tensor(np.eye(3)).numpy()\\n\",torch.tensor(np.eye(3)).numpy())\n",
    "print(\"torch.randint(0,10,(2,3))\\n\",torch.randint(0,10,(2,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3deca8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randint(0,10,(2,3))\n",
    "a = torch.randint(0,10,(2,3))\n",
    "sum = a+ b\n",
    "mul = a @ b.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c247c",
   "metadata": {},
   "source": [
    "##  AUTOGRAD: automatic differentiation\n",
    "\n",
    "* The autograd package provides automatic differentiation for all operations on Tensors\n",
    "*  If you set its attribute .requires_grad as True, it starts to track all operations on it. When you finish your computation you can call .backward() and have all the gradients computed automatically. The gradient for this tensor will be accumulated into .grad attribute.\n",
    "* To stop a tensor from tracking history, you can call .detach()\n",
    "* To prevent tracking history (and using memory), you can also wrap the code block in with torch.no_grad():. This can be particularly helpful when evaluating a model because the model may have trainable parameters with requires_grad=True, but for which we don't need the gradients.\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14019cec",
   "metadata": {},
   "source": [
    "## env\n",
    "* cart pole env\n",
    "\n",
    "We will focus on the **CartPole-v1** environment in this lab but we encourage you to also test your code on:\n",
    "* **Acrobot-v1**\n",
    "* **MountainCar-v0**\n",
    "\n",
    "| Env Info          \t| CartPole-v1 \t| Acrobot-v1                \t| MountainCar-v0 \t|\n",
    "|-------------------\t|-------------\t|---------------------------\t|----------------\t|\n",
    "| **Observation Space** \t| Box(4)      \t| Box(6)                    \t| Box(2)         \t|\n",
    "| **Action Space**      \t| Discrete(2) \t| Discrete(3)               \t| Discrete(3)    \t|\n",
    "| **Rewards**           \t| 1 per step  \t| -1 if not terminal else 0 \t| -1 per step    \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dbf5caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load CartPole-v1\n",
    "env = gym.make('CartPole-v1')\n",
    "# We wrap it in order to save our experiment on a file.\n",
    "env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2873367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2bce79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vpath():\n",
    "    for mp4 in Path(\"./gym-results\").glob(\"*.mp4\"):\n",
    "        print(mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2f706cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym-results\\openaigym.video.1.14036.video000000.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./gym-results\\openaigym.video.1.14036.video000000.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vpath()\n",
    "Video(\"./gym-results\\openaigym.video.1.14036.video000000.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bff36a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other env\n",
    "# # env = gym.make('MountainCar-v0')\n",
    "# # env = gym.make('Acrobot-v1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdcd03e",
   "metadata": {},
   "source": [
    "\n",
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5980e783",
   "metadata": {},
   "source": [
    "* Reinforce is an actor-based on policy method.\n",
    "* The policy  πθ  is parametrized by a function approximator (e.g. a neural network).\n",
    "* Recall: $$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\big[ \\sum_{t} \\gamma^t R_t \\mid x_0, \\pi \\big].$$\n",
    "* To update the parameters $\\theta$ of the policy, one has to do gradient ascent: $\\theta_{k+1} = \\theta_{k} + \\alpha \\nabla_{\\theta}J(\\pi_{\\theta})|_{\\theta_{k}}$.\n",
    "\n",
    "DId not understand 2nd point:\n",
    "* **Advantages of this approach:**\n",
    "- Compared to a Q-learning approach, here the policy is directly parametrized so a small change of the parameters will not dramatically change the policy whereas this is not the case for Q-learning approaches.\n",
    "- *The stochasticity of the policy allows exploration. In off policy learning, one has to deal with both a behaviour policy and an exploration policy*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c0c19",
   "metadata": {},
   "source": [
    "### Policy Gradient Theorem\n",
    "\n",
    "**Policy Gradient Theorem:** $$ \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}\\left[{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) R(\\tau)}\\right]$$\n",
    "\n",
    "\n",
    "The policy gradient can be approximated with:\n",
    "$$ \\hat{g} = \\frac{1}{|\\mathcal{D}|} \\sum_{\\tau \\in \\mathcal{D}} \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) R(\\tau) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a01eb0",
   "metadata": {},
   "source": [
    "\n",
    "The code is splitted in two parts:\n",
    "* The Model class defines the architecture of our neural network which takes as input the current state and returns the policy,\n",
    "* The Agent class is responsible for the training and evaluation procedure. You will need to code the method `optimize_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ad9b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dim_observation, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.dim_observation = dim_observation\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=self.dim_observation, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=self.n_actions),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        action = torch.multinomial(self.forward(state), 1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f51bc668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model we created correspond to:\n",
      "Model(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (5): Softmax(dim=0)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "env = gym.make(env_id)\n",
    "model = Model(env.observation_space.shape[0], env.action_space.n)\n",
    "print(f'The model we created correspond to:\\n{model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "558c1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = gym.make(config['env_id'])\n",
    "        make_seed(config['seed'])\n",
    "        self.env.seed(config['seed'])\n",
    "        self.model = Model(self.env.observation_space.shape[0], self.env.action_space.n)\n",
    "        self.gamma = config['gamma']\n",
    "        self.optimizer = torch.optim.Adam(self.model.net.parameters(), lr=config['learning_rate'])\n",
    "        self.monitor_env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)\n",
    "    \n",
    "    def _make_returns(self, rewards):\n",
    "        \"\"\"Returns the cumulative discounted rewards at each time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rewards : array\n",
    "            The array of rewards of one episode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            The cumulative discounted rewards at each time step\n",
    "            \n",
    "        Example\n",
    "        -------\n",
    "        for rewards=[1, 2, 3] this method outputs [1 + 2 * gamma + 3 * gamma**2, 2 + 3 * gamma, 3] \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "#         idea is we go in the reverse direction, new = old*gamma + this.\n",
    "        returns = np.zeros_like(rewards)\n",
    "        returns[-1] = rewards[-1]\n",
    "        for t in reversed(range(len(rewards) - 1)):\n",
    "            returns[t] = rewards[t] + self.gamma * returns[t + 1]\n",
    "        return returns\n",
    "    \n",
    "    # Method to implement\n",
    "    def optimize_model(self, n_trajectories):\n",
    "        \"\"\"Perform a gradient update using n_trajectories\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trajectories : int\n",
    "            The number of trajectories used to approximate the expectation card(D) in the formula above\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            The cumulative discounted rewards of each trajectory\n",
    "        \"\"\"\n",
    "        \n",
    "        for trajectory in n_trajectories:\n",
    "                \n",
    "            state, _ = self.monitor_env.reset()\n",
    "            ep_reward = 0\n",
    "            \n",
    "            action = self.model.select_action(state)\n",
    "            state, reward, done, _, _ = self.monitor_env.step(action)\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train(self, n_trajectories, n_update):\n",
    "        \"\"\"Training method\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trajectories : int\n",
    "            The number of trajectories used to approximate the expectation card(D) in the formula above\n",
    "        n_update : int\n",
    "            The number of gradient updates\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        rewards = []\n",
    "        for episode in range(n_update):\n",
    "            rewards.append(self.optimize_model(n_trajectories))\n",
    "            print(f'Episode {episode + 1}/{n_update}: rewards {round(rewards[-1].mean(), 2)} +/- {round(rewards[-1].std(), 2)}')\n",
    "        \n",
    "        # Plotting\n",
    "        r = pd.DataFrame((itertools.chain(*(itertools.product([i], rewards[i]) for i in range(len(rewards))))), columns=['Epoch', 'Reward'])\n",
    "        sns.lineplot(x=\"Epoch\", y=\"Reward\", data=r, ci='sd');\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate the agent on a single trajectory            \n",
    "        \"\"\"\n",
    "        \n",
    "        observation = self.monitor_env.reset()\n",
    "        observation = torch.tensor(observation, dtype=torch.float)\n",
    "        reward_episode = 0\n",
    "        done = False\n",
    "            \n",
    "        while not done:\n",
    "            action = self.model.select_action(observation)\n",
    "            observation, reward, done, info = self.monitor_env.step(int(action))\n",
    "            observation = torch.tensor(observation, dtype=torch.float)\n",
    "            reward_episode += reward\n",
    "        \n",
    "        self.monitor_env.close()\n",
    "        show_video(\"./gym-results\")\n",
    "        print(f'Reward: {reward_episode}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2881f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent(BaseAgent):\n",
    "    \n",
    "    def optimize_model(self, n_trajectories):\n",
    "        \"\"\"Perform a gradient update using n_trajectories\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trajectories : int\n",
    "            The number of trajectories used to approximate the expectation card(D) in the formula above\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            The cumulative discounted rewards of each trajectory\n",
    "        \"\"\"\n",
    "        \n",
    "        ###\n",
    "        ### Your code here\n",
    "        ###\n",
    "        \n",
    "        reward_trajectories = None\n",
    "        loss = None\n",
    "        \n",
    "        # The following lines take care of the gradient descent step for the variable loss\n",
    "        # that you need to compute.\n",
    "        \n",
    "        # Discard previous gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        # Compute the gradient \n",
    "        loss.backward()\n",
    "        # Do the gradient descent step\n",
    "        self.optimizer.step()\n",
    "        return reward_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e828a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config is:\n",
      "{'env_id': 'CartPole-v1', 'gamma': 1, 'learning_rate': 0.01, 'seed': 1235}\n"
     ]
    }
   ],
   "source": [
    "#@title Config {display-mode: \"form\", run: \"auto\"}\n",
    "\n",
    "env_id = 'CartPole-v1'  #@param [\"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\"]\n",
    "learning_rate = 0.01  #@param {type: \"number\"}\n",
    "gamma = 1  #@param {type: \"number\"}\n",
    "seed = 1235  #@param {type: \"integer\"}\n",
    "#@markdown ---\n",
    "\n",
    "config = {\n",
    "    'env_id': env_id,\n",
    "    'learning_rate': learning_rate,\n",
    "    'seed': seed,\n",
    "    'gamma': gamma\n",
    "}\n",
    "\n",
    "print(\"Current config is:\")\n",
    "pprint(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5629ebfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JERRYJ~1\\AppData\\Local\\Temp/ipykernel_14036/1268671432.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimpleAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_trajectories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_update\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\JERRYJ~1\\AppData\\Local\\Temp/ipykernel_14036/4187249440.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_trajectories, n_update)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_update\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_trajectories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Episode {episode + 1}/{n_update}: rewards {round(rewards[-1].mean(), 2)} +/- {round(rewards[-1].std(), 2)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JERRYJ~1\\AppData\\Local\\Temp/ipykernel_14036/2327970018.py\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m(self, n_trajectories)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Compute the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Do the gradient descent step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "agent = SimpleAgent(config)\n",
    "agent.train(n_trajectories=50, n_update=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
