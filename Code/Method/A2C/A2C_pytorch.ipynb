{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0fcd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/main/reinforcement_learning/actor_critic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bcb7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7060e9df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "# parser.add_argument('--gamma', type=float, default=0.99, metavar='G', help='discount factor (default: 0.99)')\n",
    "# parser.add_argument('--seed', type=int, default=543, metavar='N',help='random seed (default: 543)')\n",
    "# parser.add_argument('--render', action='store_true', help='render the environment')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='N', help='interval between training status logs (default: 10)')\n",
    "# args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e9e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 0.99\n",
    "N=543\n",
    "N=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8719a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1da1dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "\n",
    "        # actor's layer\n",
    "        self.action_head = nn.Linear(128, 2)\n",
    "\n",
    "        # critic's layer\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "        # action & reward buffer\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward of both actor and critic\n",
    "        \"\"\"\n",
    "        x = F.relu(self.affine1(x))\n",
    "\n",
    "        # actor: choses action to take from state s_t\n",
    "        # by returning probability of each action\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "\n",
    "        # critic: evaluates being in the state s_t\n",
    "        state_values = self.value_head(x)\n",
    "\n",
    "        # return values for both actor and critic as a tuple of 2 values:\n",
    "        # 1. a list with the probability of each action over the action space\n",
    "        # 2. the value from state s_t\n",
    "        return action_prob, state_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0db85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
