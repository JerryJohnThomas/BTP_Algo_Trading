# Chapter 1 : Introduction to deep reinforcement learning


state: true locations 
observations : just an image

highlights
* The difficulty of the temporal credit assignment problem
* The difficulty of the exploration vs. exploitation trade-off

Type of agents
* Agents that are designed to approximate policies are called policy-based; 
* agents that are designed to approximate value functions are called value-based; 
* agents that are designed to approximate models are called model-based; 
* agents that are designed to approximate both policies and value functions are called actor-critic

general
* over expectation : AI Winter
* DRL is about mastering specific tasks. Unlike SL, in which generalization is the goal, RL is good at concrete, well-specified tasks. 


weakness
1. many samples - One of the most significant issues you’ll find is that in most problems, agents need millions of samples to learn well-performing policies. Humans, on the other hand, can learn from a few interactions.
2. reward functions -  The point is if you’re trying to solve a task that hasn’t been modeled or doesn’t have a distinct reward function, you’ll face challenges.









