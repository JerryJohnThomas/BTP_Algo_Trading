# Chapter 12 
## Advanced actor-critic methods

Recap
* we did pure policy graidnet
* then introduced to actor-critic methods, the agent learns both a policy and a value function
* a rough approximation of Value function can be useful for reducing the variance of the policy-gradient objective
    * learning a value function and using it as a baseline or for calculating advantages can considerably reduce the variance of the targets used for policy-gradient updates 
    * Reducing the variance often leads to faster learning  

## DDPG: Approximating a deterministic policy
*  deep deterministic policy gradient (DDPG)
*  can be seen as a DQN for continuous action spaces
*  the agent collects experiences in an online manner
*  stores these experience samples into a replay buffer
*  On every step, the agent pulls out a mini-batch from the replay buffer that is commonly sampled uniformly at random
* Then we calculate a bootstrapped TD target and train a Q-function.
* DQN summarised in lines
    * neural network to predict the S -> Q(S,A) for all A
    * then arg max is done to select the correct acion
    * we need to also promote exploration
        * epsilon greedy and stuff : random stuff etc 
    * we have 2 instances one is online network and target network
    * https://www.youtube.com/watch?v=fnVIgAGhA08

*  will be continued. 
*  





