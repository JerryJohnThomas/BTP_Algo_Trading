# Chapter 3 : Balancing immediate and long-term goals 

Two fundamental algorithms for solving MDPs under a technique called dynamic programming:
* value iteration (VI) and 
* policy iteration (PI).


These above methods in a way “cheat”: they require full access to the MDP, and they depend on knowing the dynamics of the environment, which is something we can’t always obtain, but for now lets just roll with this

You’ll also notice that when an agent has full access to an MDP
* no need of exploration as no uncertainity

In DRL agents learn from feedback that’s
* 
* 
* 
* 















